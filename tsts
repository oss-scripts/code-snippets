def _generate(
    self, 
    prompts: List[str],
    params: dict = {'temperature': 0.0, 'top_p': 1.0, 'max_tokens': 1024,
                    'stop': ['Note', 'Please']},
    stop: Optional[List[str]] = None,
    run_manager: Optional[CallbackManagerForLLMRun] = None,
    **kwargs: Any,
) -> LLMResult:
    generations = []
    
    for prompt in prompts:
        # Parse the prompt to extract system and user messages
        messages = []
        
        # Remove <bos> token if present
        prompt = prompt.replace("<bos>", "")
        
        # Split into message parts
        parts = prompt.split("<start_of_turn>")
        for part in parts:
            if not part.strip():
                continue
                
            # Extract role and content
            if part.startswith("system"):
                content = part.replace("system", "", 1).strip()
                if "<end_of_turn>" in content:
                    content = content.split("<end_of_turn>")[0].strip()
                messages.append({"role": "system", "content": content})
            elif part.startswith("user"):
                content = part.replace("user", "", 1).strip()
                if "<end_of_turn>" in content:
                    content = content.split("<end_of_turn>")[0].strip()
                messages.append({"role": "user", "content": content})
            elif part.startswith("model"):
                # We don't include model/assistant parts in the input
                continue
        
        # Create request body with properly separated messages
        request_body = {
            "model": "gemma-3-27b-it",
            "messages": messages,
            "temperature": params.get("temperature", 0.0),
            "top_p": params.get("top_p", 1.0),
            "max_tokens": params.get("max_tokens", 1024),
        }
        
        # Add stop sequences if specified
        if params.get("stop") or stop:
            request_body["stop"] = params.get("stop", []) if params.get("stop") else stop
        
        # Adding any other parameters that might be specified
        for key, value in params.items():
            if key not in request_body and key not in ["stop"]:
                request_body[key] = value
                
        response = requests.post(
            f"{self.server_url}/v1/chat/completions",
            json=request_body,
        )
        
        if response.status_code != 200:
            raise ValueError(f"Error from vLLM server: {response.text}")
            
        output = response.json()
        
        # Parse the OpenAI-compatible chat response format
        if "choices" in output and len(output["choices"]) > 0:
            text = output["choices"][0]["message"]["content"]
            generations.append([Generation(text=text)])
        else:
            raise ValueError(f"Unexpected response format: {output}")
            
    return LLMResult(generations=generations)
